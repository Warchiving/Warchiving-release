# # ChromaDB ì €ì¥ ë° Hybrid Search(RRF í¬í•¨)
# # src/vector_db.py

# """
# ChromaDB ì €ì¥ ë° Hybrid Search(RRF í¬í•¨)ë¥¼ êµ¬í˜„í•  íŒŒì¼.

# - ì´í›„ ë‹¨ê³„ì—ì„œ:
#   - processed.parquetë¥¼ ì½ì–´ì™€ì„œ
#   - Chroma ì»¬ë ‰ì…˜ì— (id, vector, metadata)ë¡œ ì ì¬
#   - dense + BM25 í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ & RRF êµ¬í˜„
# """

# def init_vector_db():
#     """TODO: ChromaDB ì´ˆê¸°í™” ë¡œì§ ì‘ì„± ì˜ˆì •."""
#     pass


# src/vector_db.py

import os
from typing import List, Dict, Any, Tuple

import pandas as pd
import chromadb
from chromadb.config import Settings
from rank_bm25 import BM25Okapi

from .config import PROCESSED_PARQUET_PATH
from .embedder import BGEEmbedder



def load_passage_df(parquet_path: str = PROCESSED_PARQUET_PATH) -> pd.DataFrame:
    """
    processed.parquetë¥¼ ì½ì–´ì˜¤ëŠ” í—¬í¼ í•¨ìˆ˜.
    ì»¬ëŸ¼: venue_id, hall_name, review_idx, aspect, text_chunk, vector
    """
    if not os.path.exists(parquet_path):
        raise FileNotFoundError(f"Parquet not found at {parquet_path}")
    df = pd.read_parquet(parquet_path)
    # doc_idë¥¼ ìœ„í•´ indexë¥¼ ê³ ì •ì‹œì¼œ ë‘ 
    df = df.reset_index(drop=True)
    df["doc_id"] = df.index.astype(str)  # Chromaìš©
    return df


class DenseSparseIndex:
    """
    - Dense: ChromaDB (+ BGE ì„ë² ë”©)
    - Sparse: BM25 (text_chunk)
    - Hybrid: Dense + Sparse ê²°ê³¼ë¥¼ RRFë¡œ ê²°í•©
    """

    def __init__(
        self,
        passage_df: pd.DataFrame,
        chroma_path: str = "./chroma_db",
        collection_name: str = "wedding_passages",
    ):
        self.df = passage_df # ì´ ì‹œì ë¶€í„° doc_idê°€ ì‹œìŠ¤í…œì˜ ì ˆëŒ€ ê¸°ì¤€ í‚¤

        # Dense: ChromaDB
        self.client = chromadb.Client(
            Settings(
                chroma_db_impl="duckdb+parquet",
                persist_directory=chroma_path,
                anonymized_telemetry=False,
            )
        )
        self.collection = self.client.get_or_create_collection(name=collection_name)

        # Sparse: BM25
        docs = self.df["text_chunk"].fillna("").astype(str).tolist()
        self.tokenized_docs: List[List[str]] = [d.split() for d in docs]
        self.bm25 = BM25Okapi(self.tokenized_docs)

        # Dense Queryìš© ì„ë² ë”
        self.embedder = BGEEmbedder()

    # ì¸ë±ìŠ¤ êµ¬ì¶• (Dense / Chroma)
    def build_dense_index(self, batch_size: int = 512):
        """
        Parquetì— ì €ì¥ëœ vectorë¥¼ ê·¸ëŒ€ë¡œ Chromaì— ì˜¬ë ¤ë„ ë˜ì§€ë§Œ,
        ì—¬ê¸°ì„œëŠ” dfì— vector ì»¬ëŸ¼ì´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ê·¸ëŒ€ë¡œ push.
        (ë§Œì•½ vectorê°€ ì—†ìœ¼ë©´, ë‹¤ì‹œ embedí•´ì„œ ì‚¬ìš© ê°€ëŠ¥)
        """
        print("ğŸ“¦ Populating ChromaDB collection with existing vectors...")

        # ì´ë¯¸ ë°ì´í„°ê°€ ìˆë‹¤ë©´ ì´ˆê¸°í™”í• ì§€ ë§ì§€ ê²°ì • (ì§€ê¸ˆì€ ì¼ë‹¨ ë¹„ì›€)
        if self.collection.count() > 0:
            print("âš ï¸ Existing collection found. Deleting all and rebuilding.")
            self.collection.delete(where={})

        ids = self.df["doc_id"].tolist()
        documents = self.df["text_chunk"].tolist()
        metadatas = self.df[
            ["venue_id", "hall_name", "aspect"]
        ].to_dict(orient="records")

        vectors = self.df["vector"].tolist()  # list[list[float]]

        # batchë¡œ Chromaì— ì ì¬
        for i in range(0, len(ids), batch_size):
            batch_ids = ids[i : i + batch_size]
            batch_docs = documents[i : i + batch_size]
            batch_metas = metadatas[i : i + batch_size]
            batch_embs = vectors[i : i + batch_size]

            batch_embs = [
                emb.tolist() if hasattr(emb, "tolist") else list(emb)
                for emb in batch_embs
            ]

            self.collection.add(
                ids=batch_ids,
                documents=batch_docs,
                metadatas=batch_metas,
                embeddings=batch_embs,
            )

        print(f"âœ… Chroma collection populated ({self.collection.count()} docs).")


    # Dense Retrieval
    def dense_search(
        self,
        query_text: str,
        top_k: int = 50,
    ) -> List[Tuple[str, float]]:
        """
        Dense retriever.
        - ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ BGEë¡œ ì„ë² ë”©
        - ChromaDBì—ì„œ ìœ ì‚¬í•œ ë²¡í„° Top-K ê²€ìƒ‰
        return: [(doc_id, score), ...]
        """
        # 1) ì¿¼ë¦¬ ì„ë² ë”©
        q_vec = self.embedder.embed_texts([query_text])[0]

        # 2) Chromaì— ì§ˆì˜
        results = self.collection.query(
            query_embeddings=[q_vec],
            n_results=top_k,
            include=["distances"],
        )

        ids = results["ids"][0]
        distances = results["distances"][0]

        dense_results: List[Tuple[str, float]] = []
        for doc_id, dist in zip(ids, distances):
            # ê±°ë¦¬ê°€ ì‘ì„ìˆ˜ë¡ ìœ ì‚¬ â†’ ê°„ë‹¨íˆ score = -distance ë¡œ ë³€í™˜
            score = -float(dist)
            dense_results.append((doc_id, score))

        return dense_results


    # Sparse Retrieval (BM25)
    def sparse_search(
        self,
        query_text: str,
        top_k: int = 50,
    ) -> List[Tuple[str, float]]:
        """
        Sparse retriever (BM25).
        - text_chunk ì „ì²´ë¥¼ BM25 ì¸ë±ìŠ¤
        - ì¿¼ë¦¬ í…ìŠ¤íŠ¸ë¥¼ ê³µë°± ê¸°ì¤€ í† í°í™” í›„ BM25 ì ìˆ˜ ê³„ì‚°
        return: [(doc_id, score), ...]
        """
        query_tokens = query_text.split()
        # ê° ë¬¸ì„œì— ëŒ€í•œ BM25 ì ìˆ˜
        scores = self.bm25.get_scores(query_tokens)  # len = num_docs

        scores_series = pd.Series(scores)
        top_idx = scores_series.nlargest(top_k).index.tolist()

        sparse_results: List[Tuple[str, float]] = []
        for idx in top_idx:
            doc_id = self.df.iloc[idx]["doc_id"]
            score = float(scores[idx])
            sparse_results.append((doc_id, score))

        return sparse_results



    # RRF Fusion (dense + sparse)
    @staticmethod
    def rrf_fusion(
        ranked_lists: List[List[str]],  # ê° ë¦¬ìŠ¤íŠ¸ëŠ” doc_id ìˆœì„œëŒ€ë¡œ
        k: int = 60,
    ) -> Dict[str, float]:
        """
        Reciprocal Rank Fusion:
        score(d) = sum_{lists} 1 / (k + rank(d, list))
        ì—¬ê¸°ì„œëŠ” rankëŠ” 1-based index.
        """
        scores: Dict[str, float] = {}

        for results in ranked_lists:
            for rank, doc_id in enumerate(results, start=1):
                scores.setdefault(doc_id, 0.0)
                scores[doc_id] += 1.0 / (k + rank)

        return scores

    def hybrid_search(
        self,
        query_text: str,
        top_k_dense: int = 50,
        top_k_sparse: int = 50,
        k_final: int = 20,
    ) -> List[Dict[str, Any]]:
        """
        Hybrid Retrieval:
        1) Dense Top-K
        2) Sparse Top-K
        3) RRFë¡œ ë‘ ë­í‚¹ì„ ê²°í•©
        4) ìµœì¢… ìƒìœ„ k_final passage ë°˜í™˜

        return: [
          {
            "doc_id": ...,
            "venue_id": ...,
            "hall_name": ...,
            "review_idx": ...,
            "aspect": ...,
            "text_chunk": ...,
            "rrf_score": ...,
            "dense_score": ... (optional),
            "sparse_score": ... (optional)
          },
          ...
        ]
        """

        print(f"ğŸ” Hybrid search for query: {query_text}")

        # 1) dense / sparse ê°ê° ê²€ìƒ‰
        dense = self.dense_search(query_text, top_k=top_k_dense)
        sparse = self.sparse_search(query_text, top_k=top_k_sparse)

        # doc_id ìˆœì„œ ë¦¬ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ (RRFëŠ” ìˆœìœ„ë§Œ í•„ìš”)
        dense_ids = [doc_id for doc_id, _ in dense]
        sparse_ids = [doc_id for doc_id, _ in sparse]

        # 2) RRF ì ìˆ˜ ê³„ì‚°
        rrf_scores = self.rrf_fusion([dense_ids, sparse_ids])

        # 3) doc_id â†’ dense/sparse raw score ë§¤í•‘ (ë””ë²„ê¹…/ì„¤ëª…ìš©)
        dense_dict = {doc_id: score for doc_id, score in dense}
        sparse_dict = {doc_id: score for doc_id, score in sparse}

        # 4) RRF ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìƒìœ„ k_final ì„ íƒ
        sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)
        top_docs = sorted_docs[:k_final]

        results: List[Dict[str, Any]] = []
        for doc_id, rrf_score in top_docs:
            row = self.df[self.df["doc_id"] == doc_id].iloc[0]
            results.append(
                {
                    "doc_id": doc_id,
                    "venue_id": row["venue_id"],
                    "hall_name": row["hall_name"],
                    "aspect": row["aspect"],
                    "text_chunk": row["text_chunk"],
                    "rrf_score": float(rrf_score),
                    "dense_score": float(dense_dict.get(doc_id, 0.0)),
                    "sparse_score": float(sparse_dict.get(doc_id, 0.0)),
                }
            )

        return results
