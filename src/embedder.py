# [í•µì‹¬] BGE-M3 ê¸°ë°˜ Late Chunking êµ¬í˜„# src/embedder.py

import os
from typing import List

import torch
import pandas as pd
from tqdm import tqdm
from transformers import AutoTokenizer, AutoModel

from .config import (
    DENSE_MODEL_NAME,
    ASPECT_COLUMNS,
    VENUE_ID_COL,
    HALL_NAME_COL,
    RAW_CSV_PATH,
    PROCESSED_PARQUET_PATH,
    MAX_LENGTH,
    BATCH_SIZE,
    USE_FP16,
)


def get_device() -> torch.device:
    if torch.cuda.is_available():
        return torch.device("cuda")
    elif torch.backends.mps.is_available():
        return torch.device("mps")  # Mac M1/M2
    else:
        return torch.device("cpu")


DEVICE = get_device()


class BGEEmbedder:
    """
    BGE-M3 ê¸°ë°˜ ì„ë² ë”.
    - Late chunking ê´€ì ì—ì„œ: í† í° ì„ë² ë”©ì„ mean pooling í•´ì„œ ì»¬ëŸ¼ ë‹¨ìœ„ ë²¡í„°ë¡œ ë§Œë“¦
    """

    def __init__(self):
        print(f"ğŸ§  Loading dense model: {DENSE_MODEL_NAME}")
        self.tokenizer = AutoTokenizer.from_pretrained(DENSE_MODEL_NAME)
        self.model = AutoModel.from_pretrained(DENSE_MODEL_NAME)
        self.model.to(DEVICE)
        self.model.eval()

        self.normalize = True

    def _mean_pooling(
        self, last_hidden_state: torch.Tensor, attention_mask: torch.Tensor
    ) -> torch.Tensor:
        """
        í† í° ì„ë² ë”©ì„ attention mask ê¸°ì¤€ìœ¼ë¡œ í‰ê· ë‚´ì„œ ë¬¸ì¥/ì²­í¬ ë²¡í„°ë¡œ ë§Œë“œëŠ” í•¨ìˆ˜.
        last_hidden_state: (batch, seq_len, hidden)
        attention_mask: (batch, seq_len)
        return: (batch, hidden)
        """
        # mask í™•ì¥ (batch, seq_len, 1) â†’ (batch, seq_len, hidden)
        mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()
        # ë§ˆìŠ¤í¬ëœ hiddenì˜ í•©
        masked_hidden = last_hidden_state * mask
        sum_hidden = masked_hidden.sum(dim=1)  # (batch, hidden)
        # ì‹¤ì œ í† í° ê¸¸ì´ (íŒ¨ë”© ì œì™¸)
        lengths = mask.sum(dim=1)  # (batch, hidden) - hidden dimensionì´ì§€ë§Œ ê°™ì€ ê°’ ë°˜ë³µë¨
        lengths = torch.clamp(lengths, min=1e-9)
        pooled = sum_hidden / lengths
        return pooled

    def embed_texts(self, texts: List[str]) -> List[list]:
        """
        ì—¬ëŸ¬ ê°œì˜ í…ìŠ¤íŠ¸ë¥¼ ë°›ì•„ì„œ,
        ê° í…ìŠ¤íŠ¸ì— ëŒ€í•œ ë²¡í„°(list[float])ë¥¼ ë°˜í™˜.
        """
        if len(texts) == 0:
            return []

        enc = self.tokenizer(
            texts,
            padding=True,
            truncation=True,
            max_length=MAX_LENGTH,
            return_tensors="pt",
        )
        
        enc = {k: v.to(DEVICE) for k, v in enc.items()}

        with torch.no_grad():
            if USE_FP16 and DEVICE.type == "cuda":
                with torch.autocast("cuda", dtype=torch.float16):
                    outputs = self.model(**enc)
            else:
                outputs = self.model(**enc)

            token_embeddings = outputs.last_hidden_state  # (batch, seq_len, hidden)
            sentence_embeddings = self._mean_pooling(token_embeddings, enc["attention_mask"])

            if self.normalize:
                sentence_embeddings = torch.nn.functional.normalize(
                    sentence_embeddings, p=2, dim=1
                )

        return sentence_embeddings.cpu().tolist()

    def build_vector_parquet(
        self,
        input_csv_path: str = RAW_CSV_PATH,
        output_parquet_path: str = PROCESSED_PARQUET_PATH,
    ):
        """
        1) venues.csv ì½ê¸°
        2) row Ã— aspect ì»¬ëŸ¼ë§ˆë‹¤ í…ìŠ¤íŠ¸ ë½‘ê¸°
        3) BGE-M3ë¡œ ì„ë² ë”©
        4) data/processed/processed.parquet ì— ì €ì¥
        """

        print(f"ğŸ“‚ Loading CSV from: {input_csv_path}")
        df = pd.read_csv(input_csv_path)

        records = [] # ë‚˜ì¤‘ì— parquetë¡œ ì €ì¥ë  rowë“¤. ì„ë² ë”© ëë‚œ ë’¤ vecotrê¹Œì§€ ì±„ì›Œì„œ ì´í›„ì— ë²¡í„° dbë¡œ ì €ì¥
        texts_for_embedding = [] # ì„ë² ë”©ìš© ìˆœìˆ˜ í…ìŠ¤íŠ¸ ex. ê°•ë‚¨ì—­ì—ì„œ ë„ë³´ 3ë¶„

        print("ğŸ”„ Building (venue_id, aspect) records...")
        for _, row in tqdm(df.iterrows(), total=len(df)):
            venue_id = row[VENUE_ID_COL]
            hall_name = row[HALL_NAME_COL]

            for aspect in ASPECT_COLUMNS:
                raw_text = row.get(aspect, "") # ë”•ì…”ë„ˆë¦¬ì—ì„œ ì“°ëŠ” í•¨ìˆ˜ë¡œ, ì£¼ì–´ì§„ keyì— ëŒ€í•œ valueë¥¼ ë°˜í™˜ row.get(ket, "")

                if pd.isna(raw_text):
                    raw_text = ""

                text = str(raw_text).strip()

                # vectorëŠ” ë‚˜ì¤‘ì— ë„£ì„ ê±°ë¼ ì¼ë‹¨ None
                record = {
                    "venue_id": venue_id,
                    "hall_name": hall_name,
                    "aspect": aspect,
                    "text_chunk": text,
                    "vector": None,
                }

                records.append(record)
                texts_for_embedding.append(text)

        print(f"âœ… Total records: {len(records)}")

        # 2) í…ìŠ¤íŠ¸ë“¤ ì„ë² ë”©
        print("ğŸ§  Embedding text chunks...")
        all_vectors: List[list] = []
        for i in tqdm(range(0, len(texts_for_embedding), BATCH_SIZE)):
            batch_texts = texts_for_embedding[i : i + BATCH_SIZE]
            batch_vecs = self.embed_texts(batch_texts)
            all_vectors.extend(batch_vecs)

        assert len(all_vectors) == len(records), "í…ìŠ¤íŠ¸ ê°œìˆ˜ì™€ ë²¡í„° ê°œìˆ˜ ë¶ˆì¼ì¹˜!"

        # 3) ë²¡í„°ë¥¼ recordsì— ì±„ì›Œ
        print("ğŸ§© Attaching vectors to records...")
        for rec, vec in zip(records, all_vectors):
            rec["vector"] = vec

        processed_df = pd.DataFrame(records)

        # 4) Parquet ì €ì¥
        os.makedirs(os.path.dirname(output_parquet_path), exist_ok=True)
        print(f"ğŸ’¾ Saving Parquet to: {output_parquet_path}")
        processed_df.to_parquet(output_parquet_path, engine="pyarrow", index=False)

        print("ğŸ‰ Done: vector parquet ready.")
